# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Optional
from datetime import datetime
from typing_extensions import Literal

from pydantic import Field as FieldInfo

from .._models import BaseModel

__all__ = ["MonitorDetailResponse", "Evaluation", "EvaluationModelInput", "Stats"]


class EvaluationModelInput(BaseModel):
    ground_truth: Optional[str] = None
    """The ground truth for evaluating Ground Truth Adherence guardrail."""

    system_prompt: Optional[str] = None
    """The system prompt used to generate the output."""

    user_prompt: Optional[str] = None
    """The user prompt used to generate the output."""


class Evaluation(BaseModel):
    eval_id: str
    """A unique evaluation ID."""

    evaluation_status: Literal["in_progress", "completed", "canceled", "queued", "failed"]
    """Status of the evaluation."""

    api_model_input: EvaluationModelInput = FieldInfo(alias="model_input")
    """A dictionary of inputs sent to the LLM to generate output.

    The dictionary must contain at least a `user_prompt` field or a `system_prompt`
    field. For ground_truth_adherence guardrail metric, `ground_truth` should be
    provided.
    """

    api_model_output: str = FieldInfo(alias="model_output")
    """Output generated by the LLM to be evaluated."""

    run_mode: Literal["precision_plus", "precision", "smart", "economy"]
    """Run mode for the evaluation.

    The run mode allows the user to optimize for speed, accuracy, and cost by
    determining which models are used to evaluate the event.
    """

    created_at: Optional[datetime] = None
    """The time the evaluation was created in UTC."""

    end_timestamp: Optional[datetime] = None
    """The time the evaluation completed in UTC."""

    error_message: Optional[str] = None
    """Description of the error causing the evaluation to fail, if any."""

    error_timestamp: Optional[datetime] = None
    """The time the error causing the evaluation to fail was recorded."""

    evaluation_result: Optional[Dict[str, object]] = None
    """
    Evaluation result consisting of average scores and rationales for each of the
    evaluated guardrail metrics.
    """

    evaluation_total_cost: Optional[float] = None
    """Total cost of the evaluation."""

    guardrail_metrics: Optional[
        List[
            Literal[
                "correctness",
                "completeness",
                "instruction_adherence",
                "context_adherence",
                "ground_truth_adherence",
                "comprehensive_safety",
            ]
        ]
    ] = None
    """
    An array of guardrail metrics that the model input and output pair will be
    evaluated on.
    """

    api_model_used: Optional[str] = FieldInfo(alias="model_used", default=None)
    """Model ID used to generate the output, like `gpt-4o` or `o3`."""

    modified_at: Optional[datetime] = None
    """The most recent time the evaluation was modified in UTC."""

    nametag: Optional[str] = None
    """An optional, user-defined tag for the evaluation."""

    progress: Optional[int] = None
    """Evaluation progress.

    Values range between 0 and 100; 100 corresponds to a completed
    `evaluation_status`.
    """

    start_timestamp: Optional[datetime] = None
    """The time the evaluation started in UTC."""


class Stats(BaseModel):
    completed_evaluations: Optional[int] = None
    """Number of evaluations that completed successfully."""

    failed_evaluations: Optional[int] = None
    """Number of evaluations that failed."""

    in_progress_evaluations: Optional[int] = None
    """Number of evaluations currently in progress."""

    queued_evaluations: Optional[int] = None
    """Number of evaluations currently queued."""

    total_evaluations: Optional[int] = None
    """Total number of evaluations performed by this monitor."""


class MonitorDetailResponse(BaseModel):
    monitor_id: str
    """A unique monitor ID."""

    monitor_status: Literal["active", "inactive"]
    """Status of the monitor.

    Can be `active` or `inactive`. Inactive monitors no longer record and evaluate
    events.
    """

    name: str
    """Name of this monitor."""

    created_at: Optional[datetime] = None
    """The time the monitor was created in UTC."""

    description: Optional[str] = None
    """Description of this monitor."""

    evaluations: Optional[List[Evaluation]] = None
    """An array of all evaluations performed by this monitor.

    Each one corresponds to a separate monitor event.
    """

    stats: Optional[Stats] = None
    """
    Contains five fields used for stats of this monitor: total evaluations,
    completed evaluations, failed evaluations, queued evaluations, and in progress
    evaluations.
    """

    updated_at: Optional[datetime] = None
    """The most recent time the monitor was modified in UTC."""

    user_id: Optional[str] = None
    """User ID of the user who created the monitor."""
